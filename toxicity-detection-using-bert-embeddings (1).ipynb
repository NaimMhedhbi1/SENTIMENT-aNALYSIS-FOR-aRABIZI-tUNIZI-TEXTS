{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**: With this kernel, I wished to experiment with the effects of feature engineering on a model designed to minimize bias in toxicity detection using BERT Embeddings + LSTM. \n",
    "\n",
    "A series of functions used in the kernel are drawn from the original BERT Embeddings + LSTM kernel by Dieter https://www.kaggle.com/christofhenkel/bert-embeddings-lstm/. They are credited to the owner wherever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# pytorch bert imports\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "# keras imports\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import CuDNNLSTM, Activation, Dense, Dropout, Input, Embedding, concatenate, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import SpatialDropout1D, Dropout, add, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import keras.layers as L\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretrained-bert-models-for-pytorch', 'jigsaw-unintended-bias-in-toxicity-classification', 'pretrained-bert-including-scripts']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../input\"))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = '../input/bert-base-uncased-model/'\n",
    "INPUT_DIR = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\n",
    "BERT_VOCAB_DIR = '../input/bert-base-uncased-vocab-file/vocab.txt'\n",
    "MAX_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/'\n",
    "INPUT_DIR = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\n",
    "BERT_VOCAB_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt'\n",
    "MAX_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the bert encoded training and test data\n",
    "train_data = pd.read_csv(INPUT_DIR + 'train.csv')\n",
    "test_data = pd.read_csv(INPUT_DIR + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:10000]\n",
    "test_data  = test_data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for the training data\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "train_data['capitals'] = train_data['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "train_data['exclamation_points'] = train_data['comment_text'].apply(lambda x: len(regex.findall(x)))\n",
    "train_data['total_length'] = train_data['comment_text'].apply(len)\n",
    "\n",
    "# Feature Engineering for the test data\n",
    "test_data['capitals'] = test_data['comment_text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "test_data['exclamation_points'] = test_data['comment_text'].apply(lambda x: len(regex.findall(x)))\n",
    "test_data['total_length'] = test_data['comment_text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = ['capitals','exclamation_points','total_length']\n",
    "identity_columns = ['male','female','homosexual_gay_or_lesbian','christian','jewish','muslim',\n",
    "                    'black','white','psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing the weights\n",
    "y_ids= (train_data[identity_columns] >= 0.5).astype(int).values\n",
    "# Overall\n",
    "weights = np.ones((len(train_data),)) / 4\n",
    "# Subgroup\n",
    "weights += (train_data[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train_data['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train_data[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train_data['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train_data[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train_data['target'].values>=0.5).astype(np.int),weights]).T\n",
    "y_aux_train = train_data[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of continuous target columns to categorical\n",
    "for column in identity_columns + ['target']:\n",
    "    train_data[column]= np.where(train_data[column] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing(text):\n",
    "    filter_char = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "    text = text.lower()\n",
    "    text = text.replace(filter_char,'')\n",
    "    text = text.replace('[^a-zA-Z0-9 ]', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_text'] = train_data['comment_text'].apply(nlp_preprocessing)\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(nlp_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising BERT tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file=BERT_VOCAB_DIR)\n",
    "def tokenization(row):\n",
    "    row = tokenizer.tokenize(row)\n",
    "    row = tokenizer.convert_tokens_to_ids(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_text'] = train_data['comment_text'].apply(tokenization)\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1031,\n",
       " 11109,\n",
       " 2965,\n",
       " 2008,\n",
       " 2017,\n",
       " 3477,\n",
       " 2115,\n",
       " 13930,\n",
       " 1012,\n",
       " 1033,\n",
       " 2515,\n",
       " 2023,\n",
       " 6611,\n",
       " 2000,\n",
       " 2343,\n",
       " 8398,\n",
       " 2205,\n",
       " 1029]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_ids(doc):\n",
    "    doc = [str(i) for i in doc]\n",
    "    return ' '.join(doc)\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(string_ids)\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(string_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 23196.80it/s]\n",
      "10000it [00:00, 26816.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.zeros((train_data.shape[0],MAX_LENGTH),dtype=np.int)\n",
    "\n",
    "for i,ids in tqdm(enumerate(list(train_data['comment_text']))):\n",
    "    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n",
    "    inp_len = len(input_ids)\n",
    "    x_train[i,:inp_len] = np.array(input_ids)\n",
    "    \n",
    "x_test = np.zeros((test_data.shape[0],MAX_LENGTH),dtype=np.int)\n",
    "\n",
    "for i,ids in tqdm(enumerate(list(test_data['comment_text']))):\n",
    "\n",
    "    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n",
    "    inp_len = len(input_ids)\n",
    "    x_test[i,:inp_len] = np.array(input_ids)\n",
    "    \n",
    "with open('temporary.pickle', mode='wb') as f:\n",
    "    pickle.dump(x_test, f) # use temporary file to reduce memory\n",
    "\n",
    "# Removing extra variables to free up the memory\n",
    "del x_test\n",
    "del test_data\n",
    "del train_data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_func(y_true, y_preds):\n",
    "    loss = binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_preds) * y_true[:,1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained(BERT_PRETRAINED_DIR)\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets, loss_weight):\n",
    "    '''\n",
    "    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/\n",
    "    '''\n",
    "    words = Input(shape=(MAX_LENGTH,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n",
    "    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss=[custom_loss_func,'binary_crossentropy'], loss_weights=[loss_weight, 1.0],\n",
    "                  optimizer=Adam(lr = 0.001))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx, val_idx = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "9500/9500 [==============================] - 7s 765us/step - loss: 0.5900 - dense_3_loss: 0.1191 - dense_4_loss: 0.1969 - val_loss: 0.5075 - val_dense_3_loss: 0.1119 - val_dense_4_loss: 0.1380\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "9500/9500 [==============================] - 5s 512us/step - loss: 0.4675 - dense_3_loss: 0.1037 - dense_4_loss: 0.1252 - val_loss: 0.4947 - val_dense_3_loss: 0.1083 - val_dense_4_loss: 0.1371\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "9500/9500 [==============================] - 5s 513us/step - loss: 0.4619 - dense_3_loss: 0.1021 - dense_4_loss: 0.1247 - val_loss: 0.4924 - val_dense_3_loss: 0.1080 - val_dense_4_loss: 0.1359\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "9500/9500 [==============================] - 5s 513us/step - loss: 0.4614 - dense_3_loss: 0.1021 - dense_4_loss: 0.1244 - val_loss: 0.4928 - val_dense_3_loss: 0.1082 - val_dense_4_loss: 0.1357\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "9500/9500 [==============================] - 5s 513us/step - loss: 0.4612 - dense_3_loss: 0.1020 - dense_4_loss: 0.1243 - val_loss: 0.4910 - val_dense_3_loss: 0.1077 - val_dense_4_loss: 0.1356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "LSTM_UNITS = 128\n",
    "HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "model_predictions = []\n",
    "model_val_preds = []\n",
    "weights = []\n",
    "\n",
    "# Model Training and Prediction Phase\n",
    "model = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x_train[tr_idx],[y_train[tr_idx], y_aux_train[tr_idx]],\n",
    "              validation_data = (x_train[val_idx],[y_train[val_idx], y_aux_train[val_idx]]),\n",
    "              batch_size=512,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** epoch))])\n",
    "    with open('temporary.pickle', mode='rb') as f:\n",
    "        x_test = pickle.load(f) \n",
    "    model_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "    model_val_preds.append(model.predict(x_train[val_idx], batch_size=2048)[0].flatten())\n",
    "    del x_test\n",
    "    gc.collect()\n",
    "    weights.append(2 ** epoch)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = np.average(model_val_preds, weights = weights, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45289495571666605"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Following section is drawn from a set of functions used on https://www.kaggle.com/christofhenkel/bert-embeddings-lstm/ \"\"\"\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_s_auc(y_true,y_pred,y_identity):\n",
    "    mask = y_identity==1\n",
    "    try:\n",
    "        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        s_auc = 1\n",
    "    return s_auc\n",
    "\n",
    "def get_bspn_auc(y_true,y_pred,y_identity):\n",
    "    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n",
    "    try:\n",
    "        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        bspn_auc = 1\n",
    "    return bspn_auc\n",
    "\n",
    "def get_bpsn_auc(y_true,y_pred,y_identity):\n",
    "    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n",
    "    try:\n",
    "        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        bpsn_auc = 1\n",
    "    return bpsn_auc\n",
    "\n",
    "def get_total_auc(y_true,y_pred,y_identities):\n",
    "    N = y_identities.shape[1]\n",
    "    \n",
    "    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "\n",
    "    M_s_auc = np.power(np.mean(np.power(saucs, -5)),1/-5)\n",
    "    M_bpsns_auc = np.power(np.mean(np.power(bpsns, -5)),1/-5)\n",
    "    M_bspns_auc = np.power(np.mean(np.power(bspns, -5)),1/-5)\n",
    "    r_auc = roc_auc_score(y_true,y_pred)\n",
    "    \n",
    "    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + r_auc\n",
    "    total_auc/= 4\n",
    "\n",
    "    return total_auc\n",
    "\n",
    "get_total_auc(y_train[val_idx][:,0],val_preds,y_ids[val_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission Stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c7bf1bce1e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "# Calculate average predictions for the model\n",
    "predictions = np.average(model_predictions, weights=weights, axis=0)\n",
    "\n",
    "df_submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "df_submission.drop(['comment_text'],axis = 1, inplace = True)\n",
    "df_submission['prediction'] = predictions\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
