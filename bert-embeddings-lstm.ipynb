{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__output__.json', '__notebook__.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'\n",
    "CASED = 'uncased' in BERT_MODEL\n",
    "INPUT = '../input/jigsaw-bert-preprocessed-input/'\n",
    "TEXT_COL = 'comment_text'\n",
    "MAXLEN = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pip install pytorch-pretrained-bert without internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "BERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create BERT model and put on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained(BERT_FP)\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,do_lower_case = CASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(INPUT + 'train_bert-base-uncased_ids.csv').sample(frac = 1.0, random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(INPUT + 'test_bert-base-uncased_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1804874it [00:46, 38690.29it/s]\n",
      "97320it [00:02, 38836.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.zeros((train.shape[0],MAXLEN),dtype=np.int)\n",
    "\n",
    "for i,ids in tqdm(enumerate(list(train[TEXT_COL]))):\n",
    "\n",
    "    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n",
    "    inp_len = len(input_ids)\n",
    "    x_train[i,:inp_len] = np.array(input_ids)\n",
    "    \n",
    "x_test = np.zeros((test.shape[0],MAXLEN),dtype=np.int)\n",
    "\n",
    "for i,ids in tqdm(enumerate(list(test[TEXT_COL]))):\n",
    "\n",
    "    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n",
    "    inp_len = len(input_ids)\n",
    "    x_test[i,:inp_len] = np.array(input_ids)\n",
    "    \n",
    "with open('temporary.pickle', mode='wb') as f:\n",
    "    pickle.dump(x_test, f) # use temporary file to reduce memory\n",
    "\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "identity_columns = ['male','female','homosexual_gay_or_lesbian','christian','jewish','muslim','black','white','psychiatric_or_mental_illness']\n",
    "y_identities = (train[identity_columns] >= 0.5).astype(int).values\n",
    "\n",
    "# Overall\n",
    "weights = np.ones((len(train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import keras.layers as L\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets, loss_weight):\n",
    "    '''\n",
    "    credits go to: https://www.kaggle.com/thousandvoices/simple-lstm/\n",
    "    '''\n",
    "    words = Input(shape=(MAXLEN,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_ind, val_ind = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1714630 samples, validate on 90244 samples\n",
      "Epoch 1/1\n",
      "1714630/1714630 [==============================] - 886s 517us/step - loss: 0.3441 - dense_3_loss: 0.0711 - dense_4_loss: 0.1160 - val_loss: 0.3108 - val_dense_3_loss: 0.0621 - val_dense_4_loss: 0.1115\n",
      "Train on 1714630 samples, validate on 90244 samples\n",
      "Epoch 1/1\n",
      "1073664/1714630 [=================>............] - ETA: 5:24 - loss: 0.2973 - dense_3_loss: 0.0592 - dense_4_loss: 0.1073"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "NUM_MODELS = 1\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 512\n",
    "checkpoint_predictions = []\n",
    "checkpoint_val_preds = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(x_train[tr_ind],[y_train[tr_ind], y_aux_train[tr_ind]],validation_data = (x_train[val_ind],[y_train[val_ind], y_aux_train[val_ind]]),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        with open('temporary.pickle', mode='rb') as f:\n",
    "            x_test = pickle.load(f) # use temporary file to reduce memory\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        checkpoint_val_preds.append(model.predict(x_train[val_ind], batch_size=2048)[0].flatten())\n",
    "        del x_test\n",
    "        gc.collect()\n",
    "        weights.append(2 ** global_epoch)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = np.average(checkpoint_val_preds, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def power_mean(x, p=-5):\n",
    "    return np.power(np.mean(np.power(x, p)),1/p)\n",
    "\n",
    "def get_s_auc(y_true,y_pred,y_identity):\n",
    "    mask = y_identity==1\n",
    "    try:\n",
    "        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        s_auc = 1\n",
    "    return s_auc\n",
    "\n",
    "def get_bpsn_auc(y_true,y_pred,y_identity):\n",
    "    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n",
    "    try:\n",
    "        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        bpsn_auc = 1\n",
    "    return bpsn_auc\n",
    "\n",
    "def get_bspn_auc(y_true,y_pred,y_identity):\n",
    "    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n",
    "    try:\n",
    "        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n",
    "    except:\n",
    "        bspn_auc = 1\n",
    "    return bspn_auc\n",
    "\n",
    "def get_total_auc(y_true,y_pred,y_identities):\n",
    "\n",
    "    N = y_identities.shape[1]\n",
    "    \n",
    "    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n",
    "\n",
    "    M_s_auc = power_mean(saucs)\n",
    "    M_bpsns_auc = power_mean(bpsns)\n",
    "    M_bspns_auc = power_mean(bspns)\n",
    "    rauc = roc_auc_score(y_true,y_pred)\n",
    "\n",
    "\n",
    "    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + rauc\n",
    "    total_auc/= 4\n",
    "\n",
    "    return total_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90244,), (90244,), (90244, 9))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[val_ind][:,0].shape, val_preds.shape,y_identities[val_ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299482889875169"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_total_auc(y_train[val_ind][:,0],val_preds,y_identities[val_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
    "df_submit.prediction = predictions\n",
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
