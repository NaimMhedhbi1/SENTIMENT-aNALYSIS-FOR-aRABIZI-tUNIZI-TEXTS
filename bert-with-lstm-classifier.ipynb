{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/src/script.ipynb\n",
      "/kaggle/lib/kaggle/gcp.py\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-tf_model.h5\n",
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/bert-base-uncased/bert-base-uncased-pytorch_model.bin\n",
      "/kaggle/input/bert-base-uncased/bert-base-uncased-vocab.txt\n",
      "/kaggle/input/bert-base-uncased/bert-base-uncased-config.json\n",
      "/kaggle/input/bert-base-uncased/bert-base-uncased-tf_model.h5\n",
      "/kaggle/input/saved-pretrains/vocab.txt\n",
      "/kaggle/input/saved-pretrains/config.json\n",
      "/kaggle/input/saved-pretrains/special_tokens_map.json\n",
      "/kaggle/input/saved-pretrains/tokenizer_config.json\n",
      "/kaggle/input/bert-with-lstm-classifier/custom.css\n",
      "/kaggle/input/bert-with-lstm-classifier/__notebook__.ipynb\n",
      "/kaggle/input/bert-with-lstm-classifier/__results__.html\n",
      "/kaggle/input/bert-with-lstm-classifier/__output__.json\n",
      "/kaggle/input/bert-with-lstm-classifier/save_model_bert_with_cnn.h5\n",
      "/kaggle/input/bert-with-lstm-classifier/submission.csv\n",
      "/kaggle/input/bert-classifier/custom.css\n",
      "/kaggle/input/bert-classifier/__notebook__.ipynb\n",
      "/kaggle/input/bert-classifier/__results__.html\n",
      "/kaggle/input/bert-classifier/__output__.json\n",
      "/kaggle/input/bert-classifier/submission.csv\n",
      "/kaggle/input/bert-classifier/save_model_w.h5\n",
      "/kaggle/input/transformerswhlfiles/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "/kaggle/input/transformerswhlfiles/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl\n",
      "/kaggle/input/transformerswhlfiles/transformers-2.11.0-py3-none-any.whl\n",
      "/kaggle/working/__notebook__.ipynb\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import backend as keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D,Embedding,LSTM\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#!conda install -y gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gdown\n",
    "#url = 'https://drive.google.com/uc?export=download&id=1-NxLIxP1FZm9T-eC9u22ZvOqbTSnYTSi'\n",
    "#output = 'model_w.h5'\n",
    "#gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.11.0 from file:///kaggle/input/transformerswhlfiles/transformers-2.11.0-py3-none-any.whl in /opt/conda/lib/python3.7/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (1.18.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (0.0.43)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (2.23.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (3.0.10)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (2020.4.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (0.1.91)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (20.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (4.45.0)\r\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers==2.11.0) (0.7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (0.14.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (7.1.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (1.14.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.11.0) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.11.0) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.11.0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.11.0) (2020.4.5.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==2.11.0) (2.4.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install '/kaggle/input/transformerswhlfiles/transformers-2.11.0-py3-none-any.whl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(train):\n",
    "  #preprocessing for the train dataset\n",
    "  train['text'] = train['text'].fillna('')\n",
    "  train['selected_text'] = train['selected_text'].fillna('')\n",
    "  train['sentiment']=train['sentiment'].replace('neutral',0)\n",
    "  train['sentiment']=train['sentiment'].replace('positive',1)\n",
    "  train['sentiment']=train['sentiment'].replace('negative',2)\n",
    "  #sns.countplot(x='sentiment', data=train)\n",
    "  #plt.show()\n",
    "  return train\n",
    "def preprocess_test(test):\n",
    "  #preprocessing for the train dataset\n",
    "  test['text'] = test['text'].fillna('')\n",
    "  test['sentiment']=test['sentiment'].replace('neutral',0)\n",
    "  test['sentiment']=test['sentiment'].replace('positive',1)\n",
    "  test['sentiment']=test['sentiment'].replace('negative',2)\n",
    "  #sns.countplot(x='sentiment', data=test)\n",
    "  #plt.show()\n",
    "  return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysing\n",
    "train_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "train_original=preprocess_train(train_original)\n",
    "\n",
    "test_original=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n",
    "test_original=preprocess_test(test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knight', 'lap', 'survey', 'ma', '##ow', 'noise', 'billy', '##ium', 'shooting', 'guide', 'bedroom', 'priest', 'resistance', 'motor', 'homes', 'sounded', 'giant', '##mer', '150', 'scenes']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../input/saved-pretrains')\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "print(list(vocabulary.keys())[5000:5020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 35\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "  return tokenizer.encode_plus(review, \n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples_train(ds):\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  all_list=[]\n",
    "  for review,selected,sent in (ds[['text','selected_text','sentiment']]).itertuples(index=False):\n",
    "    bert_input = convert_example_to_feature(review)\n",
    "    input_ids_list=(bert_input['input_ids'])\n",
    "    token_type_ids_list=(bert_input['token_type_ids'])\n",
    "    attention_mask_list=(bert_input['attention_mask'])\n",
    "    bertselect=convert_example_to_feature(selected )\n",
    "    label_list.append([sent*x for x in bertselect['attention_mask']])\n",
    "    all_list.append([input_ids_list,token_type_ids_list,attention_mask_list])\n",
    "  return all_list,label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples_test(ds):\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  all_list=[]\n",
    "  for review,sent in (ds[['text','sentiment']]).itertuples(index=False):\n",
    "    bert_input = convert_example_to_feature(review)\n",
    "    input_ids_list=(bert_input['input_ids'])\n",
    "    token_type_ids_list=(bert_input['token_type_ids'])\n",
    "    attention_mask_list=(bert_input['attention_mask'])\n",
    "    all_list.append([input_ids_list,token_type_ids_list,attention_mask_list])\n",
    "  return all_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train dataset\n",
    "ds_train_encoded,labels_train = encode_examples_train(train_original)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples_test(test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1))\n",
    "    sum_ = tf.reduce_sum(y_true + y_pred, axis=(1))\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    jd =  (1 - jac) * smooth\n",
    "    return tf.reduce_mean(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_score(y_true, y_pred, smooth=100):\n",
    "    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n",
    "    arr1=np.array([np.array(xi) for xi in y_true])\n",
    "    arr1=(arr1>0).astype('int')\n",
    "    arr2=np.array([np.array(xi) for xi in y_pred])\n",
    "    arr2=(arr2>0).astype('int')\n",
    "    intersection = np.sum(np.multiply(arr1 , arr2), axis=(1))\n",
    "    sum_ =np.sum(arr1,axis=1)+np.sum(arr2,axis=1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth) #score\n",
    "    jd =  (1 - jac) * smooth #distance\n",
    "    return (jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3, 35)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 35)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 35)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 35)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 35, 768), (N 109482240   tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 35, 128)      459264      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 35, 64)       24640       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 35, 64)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 32)       6176        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 35, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 16)       1552        dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 35, 3)        51          conv1d_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 109,973,923\n",
      "Trainable params: 109,973,923\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel,BertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Reshape,LSTM,MaxPooling2D,Flatten,Conv1D,MaxPooling1D,Dropout\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n",
    "all_ins = Input(shape = (3,max_length,), dtype=tf.int32)\n",
    "ids = all_ins[:,0,:]\n",
    "att =  all_ins[:,1,:]\n",
    "tok =  all_ins[:,2,:]\n",
    "\n",
    "'''\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "'''\n",
    "#config = BertConfig.from_pretrained('kaggle/input/saved-pretrains/config.json')\n",
    "config = BertConfig() # print(config) to see settings\n",
    "config.output_hidden_states = False # \n",
    "bert = TFBertModel.from_pretrained('/kaggle/input/bert-base-uncased/bert-base-uncased-tf_model.h5',config=config)([ids,att,tok])\n",
    "\n",
    "#bert = TFBertModel.from_pretrained('bert-base-uncased')([ids,att,tok]) \n",
    "bert = bert[0]\n",
    "lstmlayer=LSTM(128,input_shape=(35,768), return_sequences=True)(bert)\n",
    "\n",
    "conv1=Conv1D(64,(3),padding='same',activation='relu')(lstmlayer)\n",
    "drop1=Dropout(0.5)(conv1)\n",
    "conv2=Conv1D(32,(3),padding='same',activation='relu')(drop1)\n",
    "drop2=Dropout(0.5)(conv2)\n",
    "conv3=Conv1D(16,(3),padding='same',activation='relu')(drop2)\n",
    "classifier = Dense(3, activation='softmax')(conv3)\n",
    "\n",
    "model = Model(all_ins, outputs=classifier)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=[metric])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('/kaggle/input/bert-classifier/save_model_w.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 35)\n"
     ]
    }
   ],
   "source": [
    "yt=np.array(labels_train)\n",
    "print(yt.shape)\n",
    "#y= yt.reshape(yt.shape[0], yt.shape[1], 1)\n",
    "#y= yt.reshape(yt.shape[0], 1)\n",
    "#bert_history = model.fit( ds_train_encoded,labels_train,batch_size=128,epochs=18)\n",
    "model.load_weights('/kaggle/input/bert-with-lstm-classifier/save_model_bert_with_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "215/215 [==============================] - 106s 491ms/step - loss: 0.0477 - accuracy: 0.9823\n",
      "Epoch 2/6\n",
      "215/215 [==============================] - 105s 490ms/step - loss: 0.0454 - accuracy: 0.9831\n",
      "Epoch 3/6\n",
      "215/215 [==============================] - 106s 491ms/step - loss: 0.0406 - accuracy: 0.9847\n",
      "Epoch 4/6\n",
      "215/215 [==============================] - 106s 491ms/step - loss: 0.0400 - accuracy: 0.9850\n",
      "Epoch 5/6\n",
      "215/215 [==============================] - 106s 492ms/step - loss: 0.0388 - accuracy: 0.9853\n",
      "Epoch 6/6\n",
      "215/215 [==============================] - 106s 491ms/step - loss: 0.0361 - accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit( ds_train_encoded,labels_train,batch_size=128,epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/kaggle/working/save_model_bert_with_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#url = 'https://drive.google.com/uc?export=download&id=1GR-R-rr8gKx2luKRlDs5af4o1t-YVDIp'\n",
    "#output = 'berty.h5'\n",
    "#gdown.download(url, output, quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[0.         0.         0.         0.96153846 0.         0.\n",
      " 0.         0.96153846 0.         0.        ]\n",
      "0.19230769230769162\n"
     ]
    }
   ],
   "source": [
    "Y_TEST=model.predict(ds_train_encoded[10:20])\n",
    "pred=np.argmax(Y_TEST,axis=-1)\n",
    "print(pred)\n",
    "print(labels_train[0:10])\n",
    "jac_des=jaccard_score(pred,labels_train[10:20])\n",
    "print(jac_des)\n",
    "print(np.mean(jac_des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TEST_blind=model.predict(ds_test_encoded)\n",
    "Y_TEST_blind_pred=np.argmax(Y_TEST_blind,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the output\n",
    "output=[]\n",
    "output_decoded=[]\n",
    "sent_list=[]\n",
    "i=0\n",
    "for item in ds_test_encoded:\n",
    "  j=0\n",
    "  temparr=[]\n",
    "  sent=0\n",
    "  for item2 in item[0]:\n",
    "    if Y_TEST_blind_pred[i][j]>0:\n",
    "      if (item2!=102) and (item2!=101): \n",
    "        temparr.append(item2)\n",
    "      sent=Y_TEST_blind_pred[i][j]\n",
    "    j=j+1\n",
    "  output.append(temparr.copy())\n",
    "  if (sent!=0) and test_original['sentiment'][i]!=0 :\n",
    "    output_decoded.append(tokenizer.decode(temparr))\n",
    "  else:\n",
    "    output_decoded.append(test_original['text'][i])\n",
    "  sent_list.append(sent)\n",
    "  i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last session of the day  http://twitpic.com/67ezh\n",
      "[101, 2197, 5219, 1997, 1996, 2154, 8299, 1024, 1013, 1013, 1056, 9148, 25856, 2594, 1012, 4012, 1013, 6163, 9351, 2232, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[]\n",
      "Last session of the day  http://twitpic.com/67ezh\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "z=0\n",
    "print(test_original['text'][z])\n",
    "print(ds_test_encoded[z][0])\n",
    "print(Y_TEST_blind_pred[z])\n",
    "print(output[z])\n",
    "print(output_decoded[z])\n",
    "print(sent_list[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame({'textID': test_original['textID'], 'selected_text': output_decoded})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
